"""
AI Provider Interface and Implementations.
Handles the connection to LLMs (OpenAI) and Mock simulators.
"""

import time
import re
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

# Conditional import to allow running without openai installed (for ultra-light mocks)
try:
    from openai import OpenAI, APIError, RateLimitError, APITimeoutError
except ImportError:
    OpenAI = None

from app.core.config import get_settings
from app.utils.logging import create_logger
from app.core.exceptions import AIGenerationError, RetryExhaustedError

logger = create_logger(__name__, component='ai_provider')


class AIProvider(ABC):
    """Abstract interface for AI providers"""
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """
        Generate content from prompt.
        
        Returns:
            dict: {
                'content': str,
                'tokens': {'input': int, 'output': int, 'total': int},
                'model': str,
                'finish_reason': str
            }
        """
        pass


class MockAIProvider(AIProvider):
    """
    Simulates AI generation for development and testing.
    Parses the prompt to generate context-aware dummy responses 
    that pass the Validation Engine.
    """
    
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        logger.info("ðŸ¤– MOCK AI: Generating response...")
        
        # Simulate network latency (adds realism to UI spinners)
        time.sleep(1.5)
        
        # 1. INTELLIGENT PARSING
        # Attempt to find Signal IDs in the prompt to make the test "pass" validation
        # Looks for patterns like "- SIG-001:" or "Signal ID: SIG-001"
        signal_ids = re.findall(r"(?:Signal ID:|-)\s*(SIG-[\w-]+)", prompt)
        
        # Fallback if no specific signals found
        if not signal_ids:
            logger.warning("Mock AI could not find signal IDs in prompt, using defaults")
            signal_ids = ["SIG-DEMO-01", "SIG-DEMO-02"]
        
        # 2. CONSTRUCT RESPONSE
        # This template is designed to pass the 'GeneratedTestValidator' checks
        response_parts = [
            "# FAT Test Procedure (Generated by Mock AI)",
            "",
            "**System**: Mock System Environment",
            "**Date**: 2026-01-01",
            "---",
            ""
        ]
        
        for i, sig_id in enumerate(signal_ids, 1):
            response_parts.append(f"""
## Test {i}: {sig_id} Verification

**Purpose**: Verify signal stability and accuracy for {sig_id}.

**Prerequisites**:
- [x] System powered on
- [x] Calibrated reference unit available

**Procedure**:
1. Connect reference source to {sig_id}.
2. Set input level to 0% of range.
3. Verify HMI reading matches input.
4. Increase input to 50% and 100%.
5. Record values in data sheet.

**Expected Result**:
- Reading for {sig_id} matches source within defined accuracy.
- No alarms active.

**Acceptance Criteria**:
- Deviation < 0.5%
- Update rate < 100ms
""")

        content = "\n".join(response_parts)
        
        # 3. RETURN METADATA
        # Calculates "fake" token usage for the metrics dashboard
        input_tokens = len(prompt.split())
        output_tokens = len(content.split())
        
        return {
            'content': content,
            'tokens': {
                'input': input_tokens,
                'output': output_tokens,
                'total': input_tokens + output_tokens
            },
            'model': 'mock-gpt-4o-sim',
            'finish_reason': 'stop'
        }


class OpenAIProvider(AIProvider):
    """Production OpenAI implementation with full error handling"""
    
    def __init__(self):
        self.settings = get_settings()
        if not self.settings.openai_api_key:
            logger.warning("OpenAI API Key not found in settings")
            self.client = None
        else:
            self.client = OpenAI(api_key=self.settings.openai_api_key)
            
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        if not self.client:
            raise AIGenerationError("OpenAI API Key is missing. Check .env configuration.")
            
        max_retries = kwargs.get('max_retries', self.settings.max_retries)
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"Generation attempt {attempt + 1}/{max_retries}")
                
                response = self.client.chat.completions.create(
                    model=self.settings.openai_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.settings.openai_temperature,
                    max_tokens=self.settings.openai_max_tokens,
                    timeout=self.settings.timeout_sec
                )
                
                return {
                    'content': response.choices[0].message.content,
                    'tokens': {
                        'input': response.usage.prompt_tokens,
                        'output': response.usage.completion_tokens,
                        'total': response.usage.total_tokens
                    },
                    'model': response.model,
                    'finish_reason': response.choices[0].finish_reason
                }
                
            except (RateLimitError, APITimeoutError) as e:
                logger.warning(f"Transient error on attempt {attempt + 1}: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(self.settings.retry_delay_sec * (2 ** attempt)) # Exponential backoff
                else:
                    raise RetryExhaustedError(max_retries, e)
                    
            except Exception as e:
                logger.error(f"Critical OpenAI error: {str(e)}", exc_info=True)
                raise AIGenerationError(f"OpenAI Generation Failed: {str(e)}")